{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement learning\n",
    "Reinforcement Learning involves these simple steps:\n",
    "\n",
    "1. Observation of the environment\n",
    "2. Deciding how to act using some strategy\n",
    "3. Acting accordingly\n",
    "4. Receiving a reward or penalty\n",
    "5. Learning from the experiences and refining our strategy\n",
    "6. Iterate until an optimal strategy is found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Self-Driving Cab\n",
    "Let's design a simulation of a self-driving cab. <br>\n",
    "The major goal is to demonstrate, in a simplified environment, how you can use RL techniques to develop an efficient and safe approach for tackling this problem.\n",
    "\n",
    "The Smartcab's job is to pick up the passenger at one location and drop them off in another. Here are a few things that we'd love our Smartcab to take care of:\n",
    "\n",
    "* Drop off the passenger to the right location.\n",
    "* Save passenger's time by taking minimum time possible to drop off\n",
    "* Take care of passenger's safety and traffic rules\n",
    "* There are different aspects that need to be considered here while modeling an RL solution to this problem: rewards, states, and actions (we will define these later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  OpenAI Gym \n",
    "\n",
    "Fortunately, there are previously designed game environments which we can plug into our code and test an agent. \n",
    "**OpenAI Gym** is a toolkit for developing and comparing reinforcement learning algorithms.\n",
    "The library takes care of API for providing all the information that our agent would require, like possible actions, score, and current state. We just need to focus just on the algorithm part for our agent.\n",
    "\n",
    "We'll be using the Gym environment called *Taxi-V3*, which all of the details explained above were pulled from. The objectives, rewards, and actions are all the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *,,Teach a Taxi to pick up and drop off passengers at the right locations with Reinforcement Learning\"*\n",
    "\n",
    "Let's install **gym** first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cmake in c:\\users\\tothb\\appdata\\roaming\\python\\python310\\site-packages (3.22.3)\n",
      "Requirement already satisfied: gym[atari] in c:\\users\\tothb\\appdata\\roaming\\python\\python310\\site-packages (0.19.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\tothb\\appdata\\roaming\\python\\python310\\site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\tothb\\appdata\\roaming\\python\\python310\\site-packages (from gym[atari]) (1.22.3)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\tothb\\appdata\\roaming\\python\\python310\\site-packages (from gym[atari]) (1.6.0)\n",
      "Requirement already satisfied: atari-py==0.2.6 in c:\\users\\tothb\\appdata\\roaming\\python\\python310\\site-packages (from gym[atari]) (0.2.6)\n",
      "Requirement already satisfied: opencv-python>=3. in c:\\users\\tothb\\appdata\\roaming\\python\\python310\\site-packages (from gym[atari]) (4.5.5.64)\n",
      "Requirement already satisfied: six in c:\\users\\tothb\\appdata\\roaming\\python\\python310\\site-packages (from atari-py==0.2.6->gym[atari]) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install cmake gym[atari] scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    ",,There are 4 locations (labeled by different letters), and our job is to pick up the passenger at one location and drop him off at another. We **receive +20 points** for a successful drop-off and **lose 1 point** for every time-step it takes. There is also a **10 point penalty** for illegal pick-up and drop-off actions.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the game environment and check how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.10.2)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "+---------+\n",
      "|R: | : :\u001b[43mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import pygame\n",
    "\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core gym interface is **env**, which is the unified environment interface. There are several env methods that will be helpful to us: *env.reset*, *env.render*, *env.action_space*, *env.observation_space*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[43mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "Action Space Discrete(6)\n",
      "State Space Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "env.reset() # reset environment to a new, random state\n",
    "env.render() # Renders (gives) one frame of the environment (visualizing the environment)\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The filled square represents the taxi, which is yellow without a passenger and green with a passenger.\n",
    "* The pipe (\"|\") represents a wall which the taxi cannot cross.\n",
    "* R, G, Y, B are the possible pickup and destination locations. The **blue letter represents the current passenger pick-up location**, and the **purple letter is the current destination**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As verified by the prints, we have an **Action Space** of size 6 (6 possible actions (see below)) and a **State Space** of size 500 (25x4x5=500, where 25 is the number of fields, 4 is the number of destinations, and 5 is the 5 passenger locations including 4 fields and the case of sitting in the car.).<br>\n",
    "\n",
    "As you'll see, our RL algorithm won't need any more information than these two things. All we need is a way to identify a state uniquely by assigning a unique number to every possible state, and RL learns to choose an action number from 0-5 where:\n",
    "* 0 = south\n",
    "* 1 = north\n",
    "* 2 = east\n",
    "* 3 = west\n",
    "* 4 = pickup\n",
    "* 5 = dropoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning will learn a mapping of **states** to the optimal **action** to perform in that state by exploration, i.e. the agent explores the environment and takes actions based on rewards defined in the environment.\n",
    "\n",
    "The optimal action for each state is the action that has the **highest cumulative long-term reward**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the case when the taxi is at row 3, column 1, our passenger is at location 2, and our destination is location 0.<br>\n",
    "Using the Taxi-v3 state encoding method, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(6)\n",
      "Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "print(format(env.action_space))\n",
    "print(format(env.observation_space))\n",
    "\n",
    "# Action space\n",
    "# Meghatározzta, hogy a taxi hány féle lépést (action) hajthat végre. \n",
    "# (Fel, le, jobbra, balra, utast felvesz, utast kitesz)\n",
    "\n",
    "# State space\n",
    "# Az összes lehetséges állapot számát határozza meg, ami jelen esetben 25*4*5 = 500\n",
    "# 5x5 = 25 - Játéktér mérete, ennyi helyet kell figyelembe vennünk\n",
    "# 4 - A pályán látható betűk, amik a lehetséges felvevő, illetve cél állomásokat szimbolizálják\n",
    "# 5 - Ahány helyen egy utas lehet. 4 betű, illetve a taxi ha már fel lett véve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 328\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = env.encode(3,1,2,0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.s = 328\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State2: 453\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| :\u001b[43m \u001b[0m|\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state2 = env.encode(4, 2, 3, 1)\n",
    "print(\"State2:\", state2)\n",
    "\n",
    "env.reset()\n",
    "env.s = state2\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using our illustration's coordinates to generate a number corresponding to a state between 0 and 499, which turns out to be **328** for our illustration's state.\n",
    "\n",
    "Then we can set the environment's state manually with **env.s** using that encoded number. You can play around with the numbers and you'll see the taxi, passenger, and destination move around."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the Taxi environment is created, there is an initial Reward table that's also created, called **P**. We can think of it like a matrix that has the number of states as rows and number of actions as columns, i.e. a *states × actions* matrix.\n",
    "\n",
    "Let's see the reward table of our state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 428, -1, False)],\n",
       " 1: [(1.0, 228, -1, False)],\n",
       " 2: [(1.0, 348, -1, False)],\n",
       " 3: [(1.0, 328, -1, False)],\n",
       " 4: [(1.0, 328, -10, False)],\n",
       " 5: [(1.0, 328, -10, False)]}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[328]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[42mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 479, -1, False)],\n",
       " 1: [(1.0, 379, -1, False)],\n",
       " 2: [(1.0, 499, -1, False)],\n",
       " 3: [(1.0, 479, -1, False)],\n",
       " 4: [(1.0, 479, -10, False)],\n",
       " 5: [(1.0, 475, 20, True)]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "env.s = 479\n",
    "env.render()\n",
    "env.P[479]\n",
    "\n",
    "# 3 == west == bal\n",
    "# balra szeretne mozogni, de nem tud a fal miatt, ezért nem változik az environment\n",
    "# -1 pontot kap\n",
    "\n",
    "# 5 == dropoff == kiteszi az utast\n",
    "# kiteszi az utast, megváltozik a environment 475-re\n",
    "# mivel a kellő helyen tette ki az utast, kap 20 pontot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dictionary has the structure *{action: [(probability=1.0, nextstate, reward, done)]}*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if our agent chose to explore action two (3) in this state it would be going West into a wall. The source code has made it impossible to actually move the taxi across a wall, so if the taxi chooses that action, it will just keep accruing -1 penalties, which affects the long-term reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solving the environment without Reinforcement Learning\n",
    "Let's see what would happen if we try to brute-force our way to solving the problem without RL.\n",
    "\n",
    "Since we have our **P** table for default rewards in each state, we can try to have our taxi navigate just using that.\n",
    "\n",
    "We'll create an infinite loop which runs until one passenger reaches one destination (one episode), or in other words, when the received reward is 20. The *env.action_space.sample()* method automatically selects one random action from set of all possible actions.\n",
    "\n",
    "Let's see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 113\n",
      "Penalties incurred: 29\n"
     ]
    }
   ],
   "source": [
    "env.s = 328  # set environment to illustration's state\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False # env.step(action) function can change its value\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # env.step(action): Step the environment by one timestep. returns 4 values\n",
    "                                                # done = picked up AND droped off a passenger successfully\n",
    "                                                # info = additional info such as performance and latency for debugging purposes\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 113\n",
      "State: 0\n",
      "Action: 5\n",
      "Reward: 20\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.05)\n",
    "        \n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enumerate\n",
    "# akkor használjuk, hogyha egy for cikluson belül elakarjuk tárolni az adott iteráció számát"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not good. Our agent takes thousands of timesteps and makes lots of wrong drop offs to deliver just one passenger to the right destination.\n",
    "\n",
    "This is because we aren't learning from past experience. We can run this over and over, and it will never optimize. The agent has no memory of which action was best for each state, which is exactly what Reinforcement Learning will do for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enter Reinforcement Learning\n",
    "We are going to use a simple RL algorithm called Q-learning which will give our agent some memory.\n",
    "\n",
    "###### Intro to Q-learning\n",
    "* Q-learning lets the agent use the environment's rewards to learn, over time, the best action to take in a given state.\n",
    "* The agent will learn from the P reward table.\n",
    "* Checks each action how much reward it means, takes an action accordingly.\n",
    "* Update Q-table (to remember if that action was beneficial).\n",
    "* Q-table: rows=states, columns=actions\n",
    "* Q-table is a matrix where we have a row for every state (500) and a column for every action (6).\n",
    "* Elements of Q-table: Q-values\n",
    "* A concrete Q-value: \"quality\" of an action taken from that state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory: 'img/q.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\display.py:1032\u001b[0m, in \u001b[0;36mImage._data_and_metadata\u001b[1;34m(self, always_both)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/tothb/AppData/Roaming/Python/Python310/site-packages/IPython/core/display.py?line=1030'>1031</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/tothb/AppData/Roaming/Python/Python310/site-packages/IPython/core/display.py?line=1031'>1032</a>\u001b[0m     b64_data \u001b[39m=\u001b[39m b2a_base64(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata)\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mascii\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   <a href='file:///c%3A/Users/tothb/AppData/Roaming/Python/Python310/site-packages/IPython/core/display.py?line=1032'>1033</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\formatters.py:973\u001b[0m, in \u001b[0;36mMimeBundleFormatter.__call__\u001b[1;34m(self, obj, include, exclude)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/tothb/AppData/Roaming/Python/Python310/site-packages/IPython/core/formatters.py?line=969'>970</a>\u001b[0m     method \u001b[39m=\u001b[39m get_real_method(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_method)\n\u001b[0;32m    <a href='file:///c%3A/Users/tothb/AppData/Roaming/Python/Python310/site-packages/IPython/core/formatters.py?line=971'>972</a>\u001b[0m     \u001b[39mif\u001b[39;00m method \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/tothb/AppData/Roaming/Python/Python310/site-packages/IPython/core/formatters.py?line=972'>973</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m method(include\u001b[39m=\u001b[39;49minclude, exclude\u001b[39m=\u001b[39;49mexclude)\n\u001b[0;32m    <a href='file:///c%3A/Users/tothb/AppData/Roaming/Python/Python310/site-packages/IPython/core/formatters.py?line=973'>974</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/tothb/AppData/Roaming/Python/Python310/site-packages/IPython/core/formatters.py?line=974'>975</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\display.py:1022\u001b[0m, in \u001b[0;36mImage._repr_mimebundle_\u001b[1;34m(self, include, exclude)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/tothb/AppData/Roaming/Python/Python310/site-packages/IPython/core/display.py?line=1019'>1020</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed:\n\u001b[0;32m   <a href='file:///c%3A/Users/tothb/AppData/Roaming/Python/Python310/site-packages/IPython/core/display.py?line=1020'>1021</a>\u001b[0m     mimetype \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mimetype\n\u001b[1;32m-> <a href='file:///c%3A/Users/tothb/AppData/Roaming/Python/Python310/site-packages/IPython/core/display.py?line=1021'>1022</a>\u001b[0m     data, metadata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_and_metadata(always_both\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m   <a href='file:///c%3A/Users/tothb/AppData/Roaming/Python/Python310/site-packages/IPython/core/display.py?line=1022'>1023</a>\u001b[0m     \u001b[39mif\u001b[39;00m metadata:\n\u001b[0;32m   <a href='file:///c%3A/Users/tothb/AppData/Roaming/Python/Python310/site-packages/IPython/core/display.py?line=1023'>1024</a>\u001b[0m         metadata \u001b[39m=\u001b[39m {mimetype: metadata}\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\display.py:1034\u001b[0m, in \u001b[0;36mImage._data_and_metadata\u001b[1;34m(self, always_both)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/tothb/AppData/Roaming/Python/Python310/site-packages/IPython/core/display.py?line=1031'>1032</a>\u001b[0m     b64_data \u001b[39m=\u001b[39m b2a_base64(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata)\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mascii\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   <a href='file:///c%3A/Users/tothb/AppData/Roaming/Python/Python310/site-packages/IPython/core/display.py?line=1032'>1033</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m-> <a href='file:///c%3A/Users/tothb/AppData/Roaming/Python/Python310/site-packages/IPython/core/display.py?line=1033'>1034</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m   <a href='file:///c%3A/Users/tothb/AppData/Roaming/Python/Python310/site-packages/IPython/core/display.py?line=1034'>1035</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo such file or directory: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/tothb/AppData/Roaming/Python/Python310/site-packages/IPython/core/display.py?line=1035'>1036</a>\u001b[0m md \u001b[39m=\u001b[39m {}\n\u001b[0;32m   <a href='file:///c%3A/Users/tothb/AppData/Roaming/Python/Python310/site-packages/IPython/core/display.py?line=1036'>1037</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: No such file or directory: 'img/q.png'"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory: 'img/q.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\display.py:1032\u001b[0m, in \u001b[0;36mImage._data_and_metadata\u001b[1;34m(self, always_both)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/tothb/AppData/Roaming/Python/Python310/site-packages/IPython/core/display.py?line=1030'>1031</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/tothb/AppData/Roaming/Python/Python310/site-packages/IPython/core/display.py?line=1031'>1032</a>\u001b[0m     b64_data \u001b[39m=\u001b[39m b2a_base64(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata)\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mascii\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   <a href='file:///c%3A/Users/tothb/AppData/Roaming/Python/Python310/site-packages/IPython/core/display.py?line=1032'>1033</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\formatters.py:343\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/tothb/AppData/Roaming/Python/Python310/site-packages/IPython/core/formatters.py?line=340'>341</a>\u001b[0m     method \u001b[39m=\u001b[39m get_real_method(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_method)\n\u001b[0;32m    <a href='file:///c%3A/Users/tothb/AppData/Roaming/Python/Python310/site-packages/IPython/core/formatters.py?line=341'>342</a>\u001b[0m     \u001b[39mif\u001b[39;00m method \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/tothb/AppData/Roaming/Python/Python310/site-packages/IPython/core/formatters.py?line=342'>343</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m method()\n\u001b[0;32m    <a href='file:///c%3A/Users/tothb/AppData/Roaming/Python/Python310/site-packages/IPython/core/formatters.py?line=343'>344</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/tothb/AppData/Roaming/Python/Python310/site-packages/IPython/core/formatters.py?line=344'>345</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\display.py:1054\u001b[0m, in \u001b[0;36mImage._repr_png_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/tothb/AppData/Roaming/Python/Python310/site-packages/IPython/core/display.py?line=1051'>1052</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_repr_png_\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   <a href='file:///c%3A/Users/tothb/AppData/Roaming/Python/Python310/site-packages/IPython/core/display.py?line=1052'>1053</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_FMT_PNG:\n\u001b[1;32m-> <a href='file:///c%3A/Users/tothb/AppData/Roaming/Python/Python310/site-packages/IPython/core/display.py?line=1053'>1054</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_and_metadata()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\display.py:1034\u001b[0m, in \u001b[0;36mImage._data_and_metadata\u001b[1;34m(self, always_both)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/tothb/AppData/Roaming/Python/Python310/site-packages/IPython/core/display.py?line=1031'>1032</a>\u001b[0m     b64_data \u001b[39m=\u001b[39m b2a_base64(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata)\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mascii\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   <a href='file:///c%3A/Users/tothb/AppData/Roaming/Python/Python310/site-packages/IPython/core/display.py?line=1032'>1033</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m-> <a href='file:///c%3A/Users/tothb/AppData/Roaming/Python/Python310/site-packages/IPython/core/display.py?line=1033'>1034</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m   <a href='file:///c%3A/Users/tothb/AppData/Roaming/Python/Python310/site-packages/IPython/core/display.py?line=1034'>1035</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo such file or directory: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/tothb/AppData/Roaming/Python/Python310/site-packages/IPython/core/display.py?line=1035'>1036</a>\u001b[0m md \u001b[39m=\u001b[39m {}\n\u001b[0;32m   <a href='file:///c%3A/Users/tothb/AppData/Roaming/Python/Python310/site-packages/IPython/core/display.py?line=1036'>1037</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: No such file or directory: 'img/q.png'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"img/q.png\",width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exploiting learned values\n",
    "* Epsilon\n",
    "* exploration (choosing a random action)\n",
    "* vs\n",
    "* exploitation (choosing actions based on already learned Q-values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing Q-learning in python (Training the Agent)\n",
    "First, we'll initialize the Q-table to a 500×6 matrix of zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the training algorithm that will update this Q-table as the agent explores the environment over thousands of episodes.\n",
    "\n",
    "In the first part of *while not done*, we decide whether to pick a random action or to exploit the already computed Q-values. This is done simply by using the *epsilon* value and comparing it to the *random.uniform(0, 1)* function, which returns an arbitrary number between 0 and 1.\n",
    "\n",
    "We execute the chosen action in the environment to obtain the *next_state* and the *reward* from performing the action. After that, we calculate the maximum Q-value for the actions corresponding to the *next_state*, and with that, we can easily update our Q-value to the *new_q_value*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training finished.\n",
      "\n",
      "CPU times: total: 1min 53s\n",
      "Wall time: 2min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.2\n",
    "gamma = 0.5\n",
    "epsilon = 0.2\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, 100001):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values # argmax -> action name\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state]) # max -> value\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alap értékekkel: 2perc 38mp\n",
    "# Saját értékekkel: 2perc 48mp\n",
    "# alpha = 0.2\n",
    "# gamma = 0.5\n",
    "# epsilon = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the Q-table has been established over 100,000 episodes, let's see what the Q-values are at our illustration's state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.40443843,  -2.27325184,  -2.39899733,  -2.35761809,\n",
       "       -10.67817152, -10.21467241])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[328]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The max Q-value is \"north\", so it looks like Q-learning has effectively learned the best action to take in our illustration's state!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the agent\n",
    "Let's evaluate the performance of our agent. We don't need to explore actions any further, so now the next action is always selected using the best Q-value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 13.03\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 100\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state]) # using the obtained Q-table\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1 \n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the evaluation, the agent's performance improved significantly and it incurred no penalties, which means it performed the correct pickup/dropoff actions with 100 different passengers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing our Q-learning agent to no Reinforcement Learning\n",
    "With Q-learning agent commits errors initially during exploration but once it has explored enough (seen most of the states), it can act wisely maximizing the rewards making smart moves. Let's see how much better our Q-learning solution is when compared to the agent making just random moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwoAAADdCAYAAAAb64UrAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAAEnQAABJ0Ad5mH3gAAEmiSURBVHhe7Z3/axtJnv4//5N+ahgTgcFgWP0S6YexlmyiCzfjDVkP2YzIZcQcE7ObrOaSESFjsiE4xOjWM8Ih2GQHZS54SBhNJihkgpILKy8Ze8mgQMBmFplkT3e+e3+qqqtb1d3VUrXs2JL8vOAhsdTqrq7u6no/Xd/+HwEAAAAAAACADxgFAAAAAAAAQAAYBQAAAAAAAEAAGAUAAAAAAABAABgFAAAAAAAAQAAYBQAAAAAAAEAAGAUAAAAAAABAABgFAAAAAAAAQAAYBQAAAAAAAEAAGAUAAAAAAABAABgFAAAAAAAAQAAYBQAAAAAAAEAAGAUAAAAAAABAABgFAAAAAAAAQAAYBQAAAAAAAEAAGAUAAAAAAABAABgFAAAAAAAAQAAYBQAAAAAAAEAAGAUAAAAAAABAABgFAAAAAAAAQICejcL//d//QRAEQRAEQRDU5+qVnoxCs9mEIAiCIAiCIKjPtbW1Rf/7v//bk2GIbBT4QfhBAQAAAAAAAP0Lj9n/+7//W5iFXTUKrVYLgiAIgiAIgqA+FIfH7P/4xz+EWeCtClGBUYAgCIIgCIKgIROHx+xv3ryBUYAgCIIgCIIgyBYHRgGCIAiCIAiCII84MAoQBEEQBEEQBHnEgVGAIAiCIAiCIMgjDowCBEEQBEEQBEEecWAUIAiCIAiCBlqb9PTaSZr49Vn6+m+67yEoujgwChAEQT3pJd04HqNYTKPRJE389jzd+OGl5ne7oJ9u0HGZlvNVzfdQdG0+oivvWmS9e4UebWq+h/pLbzbo6Z8v0elfJ+mALAsHDr5Hpz+/RU9fabbX6hGdl789fnOPyrKpXt5yy7z1+SP9NlC4UL614sAoQBAE9aQORsGVxQKMNc1v37JgFLahTVr74T7d/5bpLxvtz3+4RJa8ppd+ULeH+k5/+5o+OaiWQ7+S9EnZpFwOkFFgz6Nb2QMUe2eCrvywqfkesoXyHUUcGAUIgqCepBiFf56np682aENq7dtLdPgd+d07Z+n+br+hglHYhpTr+pnyZtZ9Y3ucbr1Ut4f6Sn9nwb1rEiya+JdLdGOZB4Z36cbnp2nCKZfMLJyvdguoB8koQGZC+Y4iDowCBEFQT1IqnOM36KXv+40/n7S/24s3VDAK21BIINF6Sld+wT77xRV66tke6h9t0v0/WG65O/1nTXD/N1Y2HLPwi/NdupnAKAyfUL6jiAOjAEEQ1JM6G4XW9+ft75jcYP2n+zT/+5OUHLU/t8Yn6ORnt+jphvI7T3CyRmvL5+n4uB38HDh4kq587wtYNp7Sjd8fpjEe/LwzRod/f4NW/hJuFF5+e51O/9OYbGY/QMnfnqdb/6k0wfNtbh4Xv43FztP9l/fpeu6w3c979DCdlV02eLpOHjxgp+tXZ+nGX7q8nXX6jKvH/vVpuv5tMADb/MstOv9b2bd8NEkn/3ifXrr5eZxu/KRs/+YlPbrJ0vLLCZYHbJ+/fI9OX2Pbv1G2Mc3TavuaqbKDxA36+l/Y3//yNW3I/b78nuWl0wee5b0Yl1INng+0S9q8S2flNbOU6+RX28TH6OzdTvdtiFFg9/LKMr/2E6wsWzT2rq4c29uZ3POe8sbK863P2DMiy58palB7nzZ+mKfTv7LLnDXOyuLNFdp096NJq3s/szLz4xrd/ey4/ZwQ5f4KK9vtNHBFKnd+oXwPpTgwChAEQT2pg1Fglf38B86bzdP0NQsgNquXaExWTAEdvKS82WxXetboAVnpqjpM8z/KbT3dLNpSf9c2Cpv06PMJzf64DtDJpXaf7XbgYpHldtVwxAKUfzse3E/HN3FrdMvND78sOr6kVL5/naf3AsdkFT47J/v/SiDBzv/Su85+eRDBgwn5m4Pn6dHf5XamedoxkGD5cvc6Xb9r/3+jfFruxw4UJ951AqQEXUIf8b3Rf16hhLxmp8te8+vRxtd0Wm6XuPpUv42QJvh+w+5lPhZAfG5fe8f4x95h96Y745D5Pd8ub0lKOuVZPFOUZwy7b0XA6pFFZ7917jVNWt372WLlR5OWX83TmkxDpHIXEMr3sIoDowBBENSTlEq8g5KfP5Jv/V7Src+u0N2/tgOYjW/PU1Jud/57Z7/tSi928BOar67Z4x7+46wbBDkV79oXh+VxkvTJzaf0ko+R+PERzf9rUn7eNgqbrJK0f2/R4c/v0xrf9qendMPdtl1Bq0bB2falklaerhu1lyxdL+n+Z8Hf67T5w3U6/8UjWnMM0eYK3XCCi3++IQOWDRaEyWO8c5gufWuf+8u/3qVL/+QEDM5x2t1MrA/Y7503jG823DRZf7gv8948T8O7Jqhi1/IDe5vD/77S/py37tx0rje061ICwc5d7pT7IfQacwWDb7fMsUD1vtKCsLYgzbMSfJvd82p5YwHzr/kzgpWtDR6MKvciLw/LK6KMv6xdd1sM2zMcBdOq5kfyX+fp0Y98DNUaff37hPzc6Y8fpdzphfI9nOLAKEAQBPWkLkbBaVJXm8hZYH735nW69Pl1uvHtCm280VTuugpf+/ka3fhn++92hSmlGaPw6DNZEatvEbk279NZ+ZbOOZ7aFeKRu21IutxgpHMgIbprVG/R/OeX6NIXt+gp2zZwHKXriPfcmfzHUbY9nGP75Pt19IfjMkBgwZzIf9M85Z+ZBBJKfvLuXrnzNP/n+/T0R7Qk7Km6tCis/fm6vcaAv0VBKS+O7HLjvz9WaP6X8ncfnPXec5+fpsNi2/farQom9zyTvrxxhd2Lus8193JY2exQlrqWO/U7VSjfQykOjAIEQVBPUiocZ9ajn+7SeT4gjn/Gu+IoJoG/0XffyEtZvNuC/H+7IjOt9MK2YwoYhU6VY/C7HTcKb5ipCXRNOEAT7zpvNuVxOg3C9h9HE9wF5aTJNE/5Z2aBROvvT+n6B053hLasdy953jRDuygluIxlbyljFPhbaNnt7uB5uvWFb4yCsVFo/91J4rem9zzTnhuFKOVO/c4RyvfQijMkRkG58EJ2n+DAdspbBCHdAEQIgiAjKc8d5VnS7tuqr5ys3C23eX5D6ULQ3ta00mu/3ez7FgW2jZ0nh+lKVb7p3VS7QMjjvLnvHuPwF7557gPHeUSXZLpP/rlDf3ShtxBIONrkXSceseDzLL3nDFI3+R30FuSd9Uhdw0Rn1KPPetTukpL4Y6exDUym9zzTnhuFSOVOI9NzRfkeOHGG1Cjobyx1pgMhGAUIgnqW8txRnyVvntIVZ0DiO2fprhhw1670JtQ+r0rF2a7IzCu9p390KuK3NUZBDVxC0mUQSLxccvbH8kMJzNYW3pOfO8eRM4/wz5Q+2WvVefrEHdTYPs7TqzLtfBCpM8Cb6xU7ryW1L3GPgcTxeVp59ZI23EGTbW3evUSfqH2ymda+kOfzwS3ULXul0HUUvqZLsque813nGY+4gvdH+0VAks5/r8QZLDC++8XX7nU3v+f7wChELHd+oXwPrzhDaxQCb81aazT/K982MAoQBPUstcLxPks27551WxWSclYVt9KLHRB9Xi99ftadwo+rXZFFqPTe+qxHO2MUWq++ptPSKFnjx+ns55fovDPlqpByHHWee0XJg0FD450VhU+FOEETcspWHsi1V6iNkKdMbuuLlMfccW0yM/iu871/VhSTABR6q+q6MjOXyarpmvvDM+sRv5+91/6ks3ZDhHt+740CU5Ry5xfK99CKM3xG4R1LXswJmv+rss1f52lCXmR3uj8YBQiCela4UfB8FztJt16xz95s0NObZ+mwMAcHKPlrvvbAXU1FFq3Sa718RPNvcR2FHTEKXD/dpyty7nSxfsQf79OjBX2AtPnjXbatPRWiPV/8U9oInWddzt/+a3t7Hky8l7vkm+88Yp7y/snqPO8skAjMdLK5Jud3986z7s9LaI/k3hfy+jAdOPgenf78Bs3/gQelSTq7rN4LOoXdN7wczdNZdo/yqVGFWfjtWZpf5hMUKL83vOf7wigwRSp3fqF8D6U4w2cUjl+hK3L6LbUPodtEn2Xfh1XuPz2iG5+dtJ0ju3kmQhYL2fjPW3SJuWV+0/L9iIdPYAEQVvi7LdYRoRCrDxLvgizyN4ZphyAI6mu9WaP7f35Ea2p3gL8rUy1i5VRou2L32NNuCwTuN+1WuUP5HihxhtAo3KCn5dP2/99hLpYH73wKQhnUny4/9WzrBNp8MaQJuQ13mO1mphglmWN33ObakmahISlL6bdmtFhHT0bBvyCLedohCIL6W5v09JqzNoROSTpfRYAHQTur3Sp3KN+DJs5QGoWX7iwe9qqFm9/K/sLvnKX7m75t+e+V7Y8vKH0WN+7L/r/K6oebj+j6Z/P06G/y7zebtOIstOLOn2y4WEdPRiHmXZAlStohCIL6XXw+9uUr7SZ//sw7OEEnfz9P99XnJARBO6fdKnco3wMlznAaBfaZO1gle52uy65I9pRWwW35wEPxd+wwnXYW9JA6+4HssvRv993jbYipsvj383Sr9pI6TkXYabGOnoyCr69fxLRDEARBEARBUDdxhtYotGrtFRptJehKTb+t+rY+VHLbNbat063H0YF3J9xjuQMHTRbr2AGjECXt7v4hCIIgCIIgqIM4w2sUlMWIhH45Tyth2/5wSQb0cnYSd78+KWMdDv/xkT27wZtNWvuPs0Gj4KjTYh07YBSM0w5B0HDo709p3pl6cDRJ5+9ilh0I2lOhTEJDKs4QGwW+OEZ70Ex7FUDNtsoCSdYHN2hNmb1oo3aDbjmDj1/ecrsYeebR/dsNek9+7hgFo8U63CA/Rsfn7MWSXv71Ll36J9ltycQomKYdgqAhEB//5DwfuHzTQEMQtMtCmYSGV5yhNgqtV7fopCi46tt2/baemYP43OK/tOdHFn8fvCKXeeerCrbHHhz//SW69G+n6bCzHZMwCqaLdbgDkb06MOosKGJgFJjM0g5B0MDLfSnBJy9Yoc2/40UABO2pUCahIRZnuI0CC+xv8YHM2Vu00XVbJr4+ween6T0R1POA216c5ZHaLejNS7r/x5N2IC7WRrhC93/QLG5kuFjHZm2eTv9KGoPRw/Z6DJqFRzoZBSGTtEMQNNgK664IQdDeCGUSGmJxhsQoQBAE7baUlw6fcYN/hY6LVZdjdOBXp+n695rFGv96l67n3qOJgwfkaq6dV0VuL644R3POsTxSXhw42zpjocbliwfPQpDqSqVr7irNl/gLDjXg+csK29dxe1FJ3np67amY1nnjh3l3VWdr/Dhd8Z/jT/dp/vdqGibo5Ge36KkzgYOQNw1ry+fb+XbwZHCfYmXW83SS5ZlIn/OCxh+UYdFJCGUSZRLaUXFgFCAIgnqSEpS8Y8muhaoOeNYwWVs66c4bLgISp5LlXRZY5exs1w5K1MUVuwQlf7tFJ2Ug4Befae2RuwpqOyBIHEy6aRYtoW5Qws4l0CXSotP/dpaSns+4TtPXMuDgXSDHAt9LHWRpcLtAttNgjbLgTN1O6DDN/yi3fbNGt7JOPvk1QddlX3AsOgnZQplEmYR2UhwYBQiCoJ6kBiWH6dLyij0hQe0GfeIEE7+4Qk/528Mf5+mwqCSTdP575W3ljzfouKhM2xVxOyiJeRdX5Ntruzko6Tj4Cd2ose1ZOta+vUSHZUWdcCvmdkAQe2fCnnCBbbvJ0+jum1Xk/3qDnv60QRt/cdLHt2fn+O2a2Hd7kUmluyVLx63PeHrb57fx7Xk3kDn/vbOdkgaW3vmqvU919rjjS/Zbx3ZeHKCT1+y0bvy0Qrf+8B59UpaBHBadhFyhTPLPUSahnRIHRgGCIKgnKcGAM+WxlLsavJwBZeXfJ+ztfnGczvoWRjz9K3sf78kKtV0Ra8Yj6YISd8HHYOXrzvz2DtuX6O7QDgiO3/R1AegW8KjnqFlk0v78Kd29eZ2d13W68e0KbfAppeV27eOFpcH/efvY1h/uh76BxKKTUFsok/xzlElop8SBUYAgCOpJ4UGJv9J+9JncrpPkPiIHJdpgIuy7txeUbLLf+7tBWO9OuJ9FD0o6pFWR+rY3VP6JK6AhFcok/xxlEtopcWAUIAiCelJ4UOJ/e/lySVacTrcHZVu/BvPtZXs7K3fLXT9mo3bd3S56UKLss8PbSyw6CbWFMsk/R5mEdkocGAUIgqCepFTY3fpDv/qaTst+xcnP7turuvN98JXdl+fpazcI6CEoUdMRoT/0zgYlj+iSPNbEv6+0t3tzX3M806BEzYsO/aGx6CTkCmWSf44yCe2UODAKEARBPUkNSqLNsCKmE/zlhD3VIfvbysrV2pmiByVMPcywsrNBSYueXk3K3x+gw7nzdOnzs+4Ui1y9BCUdZ1hhgaCzAi4WnYRsoUzyz1EmoZ0SB0YBgiCoJ6kVttmc7c6c5mIaRhGYnKSzX9ylFWVO856CEq7Ic7b70rfNoETMrX7zLB0WecACg1+fpRt/uas5XoSgxNmvyZzt/Pyx6OQ+F8ok/xxlEtopcWAUIAiCelJIhQ1B0B4JZRKCdlIcGAUIgqCehKAEgvpLKJMQtJPiwChAEAT1JAQlENRfQpmEoJ0UB0YBgiAIgiAIgiCPODAKEARBEARBEAR5xIFRgCAIgiAIgiDIIw6MAgRBEARBEARBHnFgFCAIgiAIgiAI8ogDowBBEARBEARBkEecPTMKjUYDgiAIgiAIgqA+FGfPjAIAAAAAAACgf4FRAAAAAAAAAASAUQAAAAAAAAAEgFEAAAAAAAAABIBRAAAAAAAAAASAUQAAAAAAAAAEgFEAAAAAAAAABIBRAAAAAAAAAATYd0aheSdHsViMEtfq8hMwjKwvTVHsN4u0Lv8GoC/YalL9doGyybh4DsVicUody1HpicEz8fUqlS9mKTXKfxcjazxN2atVWt+S3/toPilR7ugYWfw4I2OU+bhIVW2BaFHtYorts0A1+Ylga52qczmadNLK9pH+cJaqr+T3PsyPx8rnw2J7W5EHeVp8bq8C6sHJrwnftiua/Npq0OIJS1/umzUqfZyhsRG+D4vGjuao+FCTuEjH26G0ubDrcJlfhxgVHsuPVAzOQTz3RFrCJK/xq0Wa0n5vS3v8YcH0XtDQer5I+WMpiot86vDbiGV1/eGs9z76sEBlTXlorpSp8GFapj1G8eQk5Zfq1NTst/Xcu601nqHcXIUar+UGIax/NSXSMbXkO69m3XNOsdEUTX5coprmdo+STlqvUtG9Hs62q6w0+HhcsI+r1RQt+p9L/Pl1NUvpcVbu+DYsvdmLZVr1n3+E8zLP0515phrfb2+ZfWYUmlQ+FaPMkQzFErNUDym0YPCBUQB9x+s6zR61KH5slipr8hnIg82laUqx4CD/QBMoO7yuUSEZo9SZRaqv29u1GlWaYfuzTixSw/csaz3IU3wkTYV7DWrx71oNql7OkDXCKlR7sU0JD07TMkBRKrXGMk2L47EKsyHT1Vqn+pcsiAjsI8rx2K5Z2bRGMjTzQG7r5kGKBahKHsj8so7OKPnVosa9fHBbFoiXT8nK11/uX1cpP2pR+gKr0PlP+D4ezFBmxGLBkJK4KMfbqbSprMxSiqWJX4tAoG56DqHYdV/iYs0OwIRRYNcmJEAZWraRjy0WqPJrO+0EvMpvp+8pMU3EshpeHtJUXLO3YXug+jVenth291bdgLv1okJ5fiznujrwe4k9U7ILdZJJYMW3TiV+HyY7xD4/L1OO34MsEFaNQuvZLDvPOE1erdDqz/LDn+u0eIYFw6N5qrqBcsR0Nth9yI6XuVx1g+3ms0X72ePfVhgFX+AdhjTmvHxWneeXk96JIq3an0Q4L4Zxnu7QM9X0ftsF9pdReFGiTCxDpRX2sGCZ3bFiBgMNjALoO1jlVVlgAYqmkl79MtPxfm0ssO+PlGjV/1sW+BQS7Fn2nfIsa9nPt9wd/3O2RZVzrPL8aJmFjTZ2kMIqqZu+SpgFwousggo+IeU+WCXuEuF4zrae9EoaN1mZVc/ROL/k27tkgRb/5C/3Lap+6kuDpPVdnp17jpadACHK9dmRtClsrVLpCM/DMhUCRiHCOYTBAqIEr/teyL/3pVHYRj62WPCfYMHzzaCZaL1Y9bQURCqr4jokqPAwWB7W15hxkP/nNO6VqKLzMms8rlGvJS93MbI+rXp+LxDlL6Ytf/bvLEpdW6bF3/haFBoVKt3THFzct95to6RTXI9zlWA6uYHg96trlBgRjIKo/xMFb6DP2Vqn1RfK0YzPyzxPd+aZan6/7Qb7yijUryYodqosHhK1i5b7fzB8wCiAgaJjJbhO5RMxSn/pvAfzIgITJfjhQU9oi6kIGNuVdWOpQKXn7D9RK2GlbEU5Hj2ZISuWp0qwtmQ74hVugmZX5N+dUNO71aTq1VmqsAwIlPtWhfIsEJt9Jv9W2arTLK+M/V0sdETIH+O0KTjfNbZYgOA3Cts+Bzsg89R3+9EobCMfxT0+wq5p1wAtWlld/TK9A/WU/56x06A/l3XbBHwV/I6/wU6I4FpuY1IuGLWLMYqpQW4o/nTWaKaDaeH3rKeLuHEZZEH+IfP0h+E9L/M83bFnqtH9tjvsH6MgHwTZ2/LYogLLUll5g6AaCT+8ad1St3f6mk2kKX2IaWKScnP+PoiyYDy0t82MKoWE94u73P59KpkW/efq/oPzZkjRDzZFKbFtlgq361Tnb9/8DxijNOnokE7G+oMi5Y7x/dma5H2PlQpG3OSHSm5zno1deDwumSMe1la0fHAqtee83+cUjXG37hzfNH9EP8hJtv8U24ZtmzTIG+e4K3YfxszRDKX574/lqfRE88AwzH/7AcQ+5/20j8Q7PGTldbnXoArva8n2OTY6RplzyyygaNEqP29+nHGL4sksFZ8FH7jdrt3O3/OgF8Rb6NAXF3ZZmtS8XRL4KqTaZRYUXqjKv3xsVcU9FQgUIlRq9WvsnlHeAkY6njxOVXvPmAcoYqxZ4JnD9uAPxoUxCTseUfUCK4snyu3tQwg7ng7jtDmI7h7O21N/MMXY7jn8XKYse+Z6WtD3o1HYRj6Ke1z35jtAlLLa2VQY02T3Tyztefsu7rUjxWCrxvOi762+hMdHSae1w7wcOm/e3biqE4F0OnGH/NNHoLyYPqPWy+ze9uZHZDTnFTlPt/tMNbrfdod9YxTsoEd9k2Ubh8yCUqCdrklO86yL3b/TbXZihao4EafMtVp7cA4PWHlfsyS7MdzmLrsgWKMZFry2++sJx3skRXmnPy+Hv3niTdRHStROEXfVcbJOlNqBswgQp1kwz24ktRAZp0lHWDpZLs2lKX501jOwp7lSoqkRpR+uqHR8+cY/G7HI8rlicR3czwzzQeyf7WucBcOPlW1Z/vBm/a75Ix+CnmY8mTeZa7qmQIk8bvxIgSpOn0KGff5xmv5GyZQI+S+MAvt95kK53S9Si31dYslpZlbksUTzdYzi7Fq5fRcZjQX2EPO81bX7ScYnWNqVB9j6vQKlR9M041y7Hb/nQSR4H1XeH1nTR1VFVCS6IFOWAT6Yryy+tIOP0ECFlarF99l1veyrvkwrNdEkrgadEY8ngoVE+9mhIvora9IWwA5kPM9uiT+44AMzY+8vKs9UL42bk6wsznQ57/DjBTFPm43s7sHO2c4RGTypL2q2eQ7iLba/xUc82yap+A0fnOkMluwwGHsI6D0flXv8lTrwVj+41Lys2m/UeaBsD/5VrkPIYGYddgCrxg0M/lLn6iSNjU/R7O06NV7UqTyXo/Q4u+aaiRP4iwrLNUlmRoH3z+d9+XXjLnQE09mk5Y+UcTMqcoyB53qIZ1SeyvzllzJpQmAwsGsIWf3EX6Y5A4fDBjP7CD2viHm6I89Ug/ttN9gnRsEX9EiCD1DZ3HXVNyOSL5jiv9P1c+S/5w/9diVhP/Sn75kVeF5RTqrOVNPq4SAeasrDyDxNOkLSyc87pN+m6NPpFvrgg4U/FKxPizR7RGk9YPAuX4FWBj/+fBCVWpqKvDlPxTR/xBuGyY6BmBZZmep+17yd9dw7UfJfGIUzJm8LZNDge+Mifu9/gyvSquSZuGf1wafTF9z+aqfveWCCuIa88uKamKay2m9WhxxkmFbMZatRo9KZNKVPTVHavfbdKnn7+0ArllGlJgMddu+036pFP179GtvHqGL6WYW+em+GMuNZyurS5sPuA5yniqbCDxgF399+xPddzrvT8fxESRun9bBACc/2GqOwnXOQfZ0Db3zFMzHGAtIiVZ17zzGt/sHYQ0Lv+Wjfw+kTWUqzsrr4bL1937JANO0fn2NcVu1rPXUqywLAGeU6NKg2l2WmIdW9G54cDKztvsMC28q1KTEAN8WD5SQLcL/TnH1gPECnMm2n2Xl2pX9XtgeFdyMsnc4A4bmau5/mWoVmmBHIsrzyXA9hAFhdw/LVGSROrzWTJohn2RT7/ZhnkDS/BsXQwdyG52Wap5xtPlON77ddYH8YBflm2NOcyxHBkO9zHnyyB3dVuUm83TPs/m/6fnXssvO+ZW6zc/Ch78Ju8MpC3u46cihL+YUKra54gz3Rf1EbnPkfelHSpEOfTnH8sOYv0YWo3bznTQ+/0W2HLAJMp+Ln/W/ZTe45jkE+BIJgiXn+NGn5TJwsVuhK9+pdp4dzCTmuwNOMGi3/7a5HnR8fNvrrov29L62d8ibQBLyj9zyIBJ/J4nGRsqMGwZlnGkH5dukBu8OlsY7SohCYHtqgUnMH6Xl23cvxZLc535s+3irIu390alFwZgIJyyt/MGj0Fpkbfvm3n27HU4maNh7EzyT9lX6wzG/nHMQLDV/ZdnmtT6foBud58zsc9J6P9j3OB6TrWk7FdfX/zqis2i0K+jfyPIBkz+CwZzjHmV2J1QWBKylm2GFBNQuU3a6hr6pi5qXUGd51VX4mzo0PYFZT38koKLC6Wxiabi3KndLJsKcc9bWm8FYtZqL9LTwt7T1rv6xyXwRLQ6GdxUoa546Bdth5GeepZJvP1Ej321tmXxgFHqyKjA0URl1LQ8PbN82ZpcNtFrIf5PEkD2zDVJQ3R/ChL5AVhHC7cretF+yGO8aCWSXY6xRQeiudKGnSoU+nOD6rxPX7s1V8IjcWpku+3edBq1M5iVYJdsPzvPcHpIb5EBawm+ePzfqzMpUu5Cgzzrsx8fmPu/Sv72QUPHkWLf93wyh0PoZ/vzt5zwP7WrD8V9XleosuedpnVHeEUVMqVJMxA4GKskul1viKVZwjSpc1hZ6Op8U2pLqBlpzW4xlKj8Qp+1V4CBso9yb90kOCMZPjOfSSNtGy4ntGBcsmo+dzsMt1oLWwG55gdojYxr3An6ehBjZCfnnLqgzIQ+53XaDswoLvmQmL4qfKGpPBX4xpWok5W+x3CcvtNsvH01gJVu49xdrQKAhCWqQdOqazM+Jll8H4IYGaV7IuDLse3V5G2PjPyzxPXbbzTN2h+22nGH6jwPunMwfpqbQD8nZfUbuV+LuYOJWZWcWnD/TEw0IXFPiCPdHXMeTB5a10oqRJhz6dHd9KB7ArJZ4Gnra2+eKf260I/HxUU2aaD4G/Jeb5o+HnOpX4PMu+7mgeQo4r0LQomOb/bhiFSC0KjJ2750FPdLrXuiBmcFNa/kLLFUd019Mcp0Olxruq8XnRZzWD5Tk9HU+HyAPvfenQZIEAf1s/fadz0B4o9z3OdGN6PE5vabODMX195EjmW6+z9Yi8140/6sKwGoVtzHokBqiHtbKslUR3IpP88pdVf52o4jUVCk1mvsV8/Jq32AJ9veGg1h/i/4H7zquw/TiE1rVd09kJu3wYD/T2mCp+LdUXXSq89cHEKPjPyzxPXbbxTN2p+22nGHqjYA9i7vCwlHPgevpYu29UWTDJgl9//2vRQsFuoO43v/7mEk2guq4a/mBBPOjNxyiYpUlHSCHo0M9dh0jDR7M0K7sdOdjdj9jnvsJrnA9hQVSE/NHSrWlQHNd8jIJp/u+GUeh07bxjFCQ7ds+DUJq1kHm0WfazoMA/8N9LixovNHczC+rEPOzqWyl5LYPGjg8eZOZYZyB1ZWGrSVW+cNBolhY7BZtRj/dzQ7uSafUCM/6aMttYkm/eHnavN4JBi/1mUHfO4m2qZgxWlONtL206dGU++jnw3/DuGKEzaTXKlL+q7woyrF2PestHyWtmMkb06x0E8ytCWXXWFghktq87jcOLRcryBeMuVz2TjnixX9pldEG2Zt2DIMEWheaTRVrUBrUynf76yCidnCbLq+B+xdgdX53fejxLeW2LXTCvxDXR1VfiOrbjEPPz6iFPt/NMjXS/vX2G3CjYD4bwaQdtRH9sX8aLzxIJfSAqZoCxKPVpxdN1pdWoUGEiQ8UV5+KGBOBisFOC8t8oNxYfJHOBr+anFg7Zt9541iOTNOkISSeDz3pkJfOkzpzDB1vxtGbmfEGPCE5ZEOvvExv2uWk+hBkFnj88EPHlT+NewZs/6+w441kqPVPuAnac5XPhb3ME4rh81qNpKq+1t3JmPfK8sYiQ/7tiFNhZGc16pLAz9zwIRV2Z06kc+f2qWRl2/WGJZhaUrnHNCk2zbdRueqErmDL4CxIewKorJYtyFTa7kr9Se80qv1N8XA+7f3z1n44ox1v9U5rETF5OeQxbxdl5FrBKtRRlBhh/MK5bjZeXA1+eRzreTqUtQMiz2PQcHMQzN+TtOYcZBRHI/c63SuwQD2YWGOUjr2OLVLy96ilXdn9yZQXlsPyKVFblYFa1POgG6DLWv+Pp5CsDe9OlQ13VV11FWKw43HWWuqBRaNyxz3N6odY2+SHlNko6aa1IaZlOYShCnoccp3ufZxXlkLzyrI4ttxXdmn2rY0c5r8h5us1nqvH9tgsMt1HQzR+tQ/dQlcFtaCDJKgrvnPJ8jvoZexCOS3gALgbwnBij+Lj8PR/Ac68YDIjZcWoLeXYcuU7ARJR1FHRp0hGeTo5/Lv70sRzNsDQE3xTYXVSCeWa7cV1eGuVDqFFgePInQ5mj+vyxp59L09i4PeYilYwwRkGuoyDSF2kdBX3+745RsOm2joKHHbnnQUecPHQG8qoDHV1aVL3Avh8teI31epVm+T0sBkjyMSOTlF8Kr4ybT0qeaQQz/NqHVVC+Sk2MOxDpC5Em4DU/HgvElliZdfLAmeLQt61ocVSPGZBSCUtCg/FmjUpdphmMcrwdTZuHDs9ig3Nw0L38CsDXsOHPtXHnWvOBpLNU6TYD16DTNR/5AHwWUGoGPq8/nG0PwncG3uqMYqSyapcHPnZO7FdXduQsVfZx9fLfM26dJ9NgsTpWu1ZTgJAxCmKqzklKjcpjOulU65Me0tl6vkj5Y85gZnY9WIwzGzYFqC4N55jZ1W3OAvPFc8515ucfUuebnJckUp7uwDPV+H57ywx5i8LwIiqCPlqQYyjpZFAAAAAAAIYcGIV+plWj4rEclXxvbJuPZykzkqKZJ7AJbxUYBQAAAADsY2AU+pzWi4powkzxLi+iy0zK7u6xB81P+w4YBQAAAADsY2AUAAAAAAAAAAFgFAAAAAAAAAABYBQAAAAAAAAAAWAUAAAAAAAAAAFgFAAAAAAAAAABYBQAAAAAAAAAAfbMKDQaDQiCIAiCIAiC+lActCgAAAAAAAAAAsAoAAAAAAAAAALAKAAAAAAAAAACwCgAAAAAAAAAAsAoAAAAAAAAAALAKAAAAAAAAAACwCgAAAAAAAAAAsAoAAAAAAAAAALsO6PQvJOjWCxGiWt1+QnoL2pUYNen8Fj+2e9sNWj5XIbGxlNMeaoMqgd+tUhTsSlafCX/1tKixuMq1dflnwAAAAAYavaZUWhS+VSMMkcyFEvMUn1Lfgz6iMEyCo0Fdi/9ZpEag34vBYxCk1YfVGn1Z/knp7lM0yMxSs2tyg8AAAAAMMzsL6PwokSZWIZKK1XKj1iUf9CSX4D+YZCMwjqVT8Ro8qa9zPlAEzAKA9ayAwAAAIAdZ18ZhfrVBMVOlYkfvXbRcv8P+onBMgqLv4nR1NIQ9MWBUQAAAACAj/1jFLbqNJuIUfa2PPazWUrEslRWulaoRsJP60GeLHX7rXWqzuVociJN6UNME5OUm6vSuqcLigy2HtrbZkaVwKtZp/Ll9u9TyTRlL5ap7j/4VpPqtwuUnUhRSmybpcLtOtVvTokuL54Q1ShNOmQ6H7DfL+Qpy3+bjFM8yY51T31bHhY8BgPm2sUYxS5WqHFv1t7feJzGjuZpme2u9bxMhQ95+sbIGk1Rdq5O7bYdJS38XERa2Lmz855ln+lYf1Ck3DG+P1uTHxep6utrb6eH5cVDtu2ROPt/TX6jp7nipFHqwwKVV5SL86Qo8nhsJEbWOPue/b/4RH7nxwnCV9g1v5ilzNGMOKf0sTyVnmjOid8bbDtxXH7+E/Y1b+rura7XjGFyrylGYf3ONNsuRXG2/zjbNn1ompZFMvXXv2tecZx7U1xLnoaU9joBAAAAoH/YN0bBDvTzVHEjUts4ZBaUoMrpmvRC/u1ij22wPq3aAS0zHcWJOGWu1drBGw/ov5wiK1mg2mv5mQysrNEMC/RWlUBvlUpHUpRnAV1L+X31YopiR0rUTlGLqp/GyTpRagd1Wy1avT3NTIflNQrGadIh0zk+RcXH7WvTelygVCxFxefyg5BAMdQosN9OLzkBLjuXC8yIjbI0nllsnw/L86lYgmafyb9D0kKvKpRPWjS54O0fX59LU/zoLNWUTZsrbJ8jKZZOxX7w9IywY18oe/vda2h8laX4aJZKSrDL95llac9+pQbhhi0KIgi3KH6kQJVGO012OuM0/Y2S+J+XaZqZp+nbynFYkF35NMXuA3UshOk1M7zXjFoUgp81v5lmZmKaymqZkddqaql9DvVr/vSz3z5j539klurtLAEAAABAH7FPjIIv0JeIgaieQc08MLcocdU3I5LPQPDfWR8ta1oeWlQ5Zynmww6spu8ZRkKNRZpUgzVNq4dD4+akxyiYp0mHDAAfyj9dGrT4vhoI64JHTohRuFCVf0lYEBuLFaiqBIvB34alheHPD35dRnK0rMmf1nfMGCqBsEjPmYrn+mtp8fErCXb84JathwVKjOSp6n4VxShM0qLmEjRvZ5V70L7/UroZubZ4wK+OqzG9ZiH477VejILIqxTNrsi/Vdb4tXHyaojGcgAAAAD7iP1hFOQb3UCAKwyA73MejHqCQX+XJBawHYpR/rtgIMnhAWrsUIltxQkLrBmvG1RxuowcylJ+oUKrK95gbfXLNMW0wT8LvZbUrkdR0qQjLJ0hQXzX7diWoquPr3uPNAreT02PwbFbgZzzFPlzLiT4b1UoH0tTac3+U5seDSKveOAu//biPb7uvLUEgnCF5jLlnHT60uzHe77m18LkXuvFKHS+r7z3pGh5GEnTNDt2/YX2igEAAACgz9gXRiHYcuCga2loUOmIMpZBvDUNvsm1+26Hqci2am8bCOZaNZpJWpS5XCWnJ0rrRZVmjsXJUoK1TsGt1yhESZOOkHQaB/G7ZRS824pjjKY059qWM27A1Ch489WPP63B89bSySio5ysN7ZgzNkGn3y3LtBleC8N7rRejIPJqZEyfTqnpO0revKpTeaFAuaNjZLHfZTBGAQAAAOhrht8oyEHMfJG1cHm796jdQbxdQzj2m9LcHZNz0Adz7ltrv3HxBWv1a4lILQpmadJhGHQab8e2fCtGQdOiEJI/fkyNwt63KCT0XXkCmF0L03utF6PQOa+64Iyf8bXeDRd8PFGBssm4/ZzhpurDWarqbpdXVSp+PEmpUfuZxAfIZ6+GT0TQfFKyDZfcrzBdXW7DAK/KNDXCfu83xuJesNOhk75sAjAgNGtU+jgjJqKI8RczR3NUfGhWeHak3AEwYAy9UbAHMesGKEtEi4FvULPbilAXrQv+/v3mi2zpg7n1r1iQr+uy4Q/WIo5RMEuTDrOg093OoF/8to2C6RgFnl8GXd9NjUI/jFFIXPCOpdFjds2M77UejEKnvDIj7ByGgRa751IUS07T4rN1eyB5q0HVyxlmjrz3bOPOtBiAPr1Qa7f6rLNnzwkrMACcw59pvBsXn92q034706Tlj9j+2XNObxTUewGAIeE1e2aNWpS+ULHL2laLGg9mKMPKgTr5go6dKXcADB5DbhTswKvbegliDIJntiH5WSKhD9TFDEMWpT6teN74tRoVKkxkqLjiBE4hgdDPy5RjAVb+G6V65jPbXEh7u4PwyvxMlFmPTNKkwyzotIOLGFnnKu2ZldjDsnIhwx6gO2sUrPEclZ4pV03OpJOe876/5rMeWck8VdSgRqQpTRll2lVjo8B4e7MeTVN5rX0dnFmPPAv/iVmPWKX1pXc6VD5DUDaZk9OUcgyvmem9pjEKM+yaioH47J6zZ0wKHtMeezDlySvRWrCQpdTHTjepdXbfjFF2wXtO69/kA+OBhgZnAoTAeBN79i9ehpzTbj1bZGZCkwmv2T3PAhjPNZYvMYKth/akBfoJDYLw1iArOUvLummWYRTAUGLHA7oyIspDyMQYgh0qdwAMIsNtFH4uU5YFaF1XYBaVujpFJ0N8FpwpySWwZgGfv3/GN398WDDHHi98LYETYxSXc/CLuefvFYMVNDtObSHPjiPXUZBz6puto6BLkw5To8BYr9Lsh2kaG7X7ptv7rwe223aLgrOOwtEMZZx1FPibHLmVin8dhfSxHM341h2IYhQ4RmsD6PJHhxN4yXUUxLWJtI5CmrLnSr7+/ObXzOhe0wSHja9ylGLmKM7Sao810B/Tk1civXzAtK/bjHNO42NiHQV+/sM8RiG0JYcjni0sKOn6GNRcy7CuZBzR4mYQ4LfYdUzYLUHaMTkwCmAYcbp2qvW8g+yiHPYs35FyB8CAMuQtCsOLaPEIm/EH9BcIvPYdIgB/f9HTStkmzOT58Y+JYb+8bAWnHXbYqor9Tn2lD3YcxJoW8tkBowD2DU9myIr5p+duU70Qo9iJsrcsSHai3AEwqMAo9DOtGhWP5bzdOhjNx7OUGUnRzBPYhIEAgdf+YyX8LaO9KF73wIJv5x8T03k9CnuskHW5Q8uZWN9iisoyXeFGYZKK3/CWInuF7lgsTqljeVrs2joJQH8iWvlCzTsrPXzc38iMr8WbswPlDoABBkahz2m9qIiuPineVeWQ/a/o7vMcJmFggFHYh8jBwhMFKq/J512rQbWFadE1a+pQeDcHwesaFZIxynypdl7q1tXN/j60i90WC2jY9+o+tUZhvczu1xilPixS1VnzorVO9SV70LW64jkAg4L2XlcQ3we6xnK2We4AGHBgFAAA4G0gxwxlxi0WgPAB+hnKzfGxG3KWsLAWBf69dsYjszebCd3K3gz+RpUPYFb7WYcGT6/1ZmD1y0xg4gcABgGjFgXtdM/bK3cADDowCgAAsJuIQZUWzcjFAD0wk1A+FSdrYoZqr+VnCiZ9pbXrqcjZr/ytAd3esgZoLNJkbIrKxj8AoE8wGaMQMntRz+UOgCEARmEgaVHjcZXqu15Zd2uCBQB05XGBBSx5qvhf2jstCUdnqa4xCZyeZ18RM47xsQYdZNJ1AkYBDCqY9QiAnoBRGESayzQ9EqPUnHbyxbcIjAIAprQajeDKynKcQIIF5R6f0KzSzIRF8VPBBdY8hM7nLsdERJzPXdui0ChT/qovfRJ0PQKDS/g6Cs07uR7XUeit3AEwSMAogAjAKABgRpMqv+OtAzNUdZZb/rlOi2f4as0FT7ei1nO+mJ+9WmzAWGjQrRArFtDzrBDLF2YsUvH2qjbgdwgzCiI9vytRrb1UNAYzg8FHtzLzvQKlmQlor8ysLztm5Q6A4QNGAUQARgEAY/hg5qt8kTl7MHNsNEWT5xZp1dOtyF4BO9ANSJGuvDWflCh3dIwsvs3ImL14nTfap8X3mVHpMHiTEzpGwV0gT6adT4/64SxVnFmQABhUmjUqfZyhMVHuLBo7mqPiQ7UEhJed7uUOgOFj+I2CMzWlXBU3czQTcVVceyVkdZVfXrmLBZPYw0XMajLaffEk/+rBk5pVae3Vg6ty2wxLa4pSyTRlr1ao4QsuxPHdY/I3IAXKOqs38/PTriRslg6Bkw9yWlZ7f8EVmDlmqxgDAAAAAIBBYp8YBYviRwpUcZrRGc2VEk2NxGn6GyUtPy/T9GiKpm8r7xG21qnyaco3VaEdqFujGWYiVn0mIkh9Lk3xo7NUUw5lH9/bjC+MwsgY5RYUY+Icf6KoDKTyGgXRvzJZoKqy/1ajQvmJHC27MX2LapdZOiZYPijGYJ03u46maUbtTiDmcLdo6kslHa1VKv8uQ/ERr1FofjNN8eQ0lV/IDziv2LH5792mXAAAAAAAMGjsE6Mwqe1D2LydVWYysAc6pXRzIW+tUumIRfkHTjBtB+rT9wya4V/wlVD1g6T4TAqWMjBQGAXdFGxyRobsbSffvEah49RtDjwdvGVFkw+Nm1OeAYr1qwmKnSprBmfZ80W7RkEM8ErR7Ir9pwexAqy6qiwAAAAAABgk9olRCJm6rLlMuViaSmvs/2LqNPl/Datfpil2riIHN3kD9U54f+fDd0y765F+isL6NRa8hx1/pUjpkThNXi1TbW3dHmjlQ6QjbGYGNR+ImaJDYXNCe8coiCnjDpXYL3TY+8l/B6cAAAAAADCI7G+joAbcsovSGO9jz/vk6/S7ZTnoz9woiOB/VPbzD1FRLrzUySh4Bx1qjt9cpepSkfIfpiguBh56xwl02rd3f53OzWsURJpGxrTn5Gj6jmx9AAAAAAAAAwVaFDwtCgl9N5oA5kah45t8Hz23KPgRU77lKRXLUEmOHTBvUbC7ORm3KGiXvAcAAAAAAIPOPjEK5mMUEheq+m5CHsyNQqexAX6EUehhjIIe31Smb22MQoIKD9G9CAAAAABg2NgnRoHPejRN5bV2QOvMetQeoMwQsx75ZvthNJ+VKJtUZxCKYBQYfNYjK5n3zDbkLNaSmau7xkQYhZExmpqrBWc9er9Eq26a1OO3qH4t015ARtJcKbLzbrco8O2MZz1i+ZDjC9B4Zj3i6Q2Z9WhkikrqdKhbTaovZCn1sdNVCwAAAAAADBr7xChMuesoiL7zkdZRSFP2XMm31kA0o8Dxr1+QPpajGd/6DHbXI2cdBX58w3UU+MJOczmaTI7ZYyzY+fnHKDgYr6OwXqPSuUk7r45mKGO6joLIsyzlF9h5KOcGAAAAAAAGi/1jFHTBcJ/RecAxAAAAAAAAuweMQh8BowAAAAAAAPoFGIU+AkYBAAAAAAD0C8NvFAAAAAAAAACRgVEAAAAAAAAABIBRAAAAAAAAAASAUQAAAAAAAAAEgFEAAAAAAAAABIBRAAAAAAAAAATYM6PQaDQgCIIgCIIgCOpDcdCiAAAAAAAAAAgAowAAAAAAAAAIAKMAAAAAAAAACACjAAAAAAAAAAgAowAAAAAAAAAIAKMAAAAAAAAACACjAAAAAAAAAAgAowAAAAAAAAAIsO+MQvNOjmKxGCWu1eUnoL+oUYFdn8Jj+We/s9Wg5XMZGhtPMeWpAg8MAAAAgCFhnxmFJpVPxShzJEOxxCzVt+THoI8YLKPQWGD30m8WqYF7CQAAAABDxv4yCi9KlIllqLRSpfyIRfkHLfkF6B8GySisU/lEjCZv2sucAwAAAAAME/vKKNSvJih2qkz86LWLlvt/0E8MllFY/E2MppbW5d8AAAAAAMPD/jEKW3WaTcQoe1se+9ksJWJZKv9s/8lRjYSf1oM8Wer2W+tUncvR5ESa0oeYJiYpN1eldU8XFBn0PrS3zYwqAXCzTuXL7d+nkmnKXixT3X/wrSbVbxcoO5GilNg2S4XbdarfnBJdXjwhqlGadMh0PmC/X8hTlv82Gad4kh3rnvq2PCyIDwbMtYsxil2sUOPerL2/8TiNHc3TMttd63mZCh/y9I2RNZqi7Fyd2m07Slr4uYi0sHNn5z3LPtOx/qBIuWN8f7YmPy5S9ZX8UmKnh+XFQ7btkTj7f01+o6e54qRR6sMClVeUi/OkKPJ4bCRG1jj7nv2/+ER+BwAAAAAwBOwbo2AH+nmquBGpbRwyC0og7HRNeiH/drHHNlifVu2AlpmO4kScMtdq1HSCcB7QfzlFVrJAtdfyMxn0WqMZFtyvtrelVSodSVGeBeEt5ffViymKHSlRO0Utqn4aJ+tEqW0gtlq0enuamQ7LaxSM06RDpnN8ioqP29em9bhAqViKis/lB1GNAvvt9FJdpoedywVmxEZZGs8sts+H5flULEGzz+TfIWmhVxXKJy2aXFiVH9jU59IUPzpLNWXT5grb50iKpVOxHzw9I+zYF8q0qphDHY2vshQfzVJJMQZ8n1mW9uxXqnFCiwIAAAAAhpd9YhR8gb5EDET1DGrmgblFiau+GZF8BoL/zvpoWdPy0KLKOUsxH3bQO31PPWoHGos0GZuiRedtuKbVw6Fxc9JjFMzTpEMagIfyT5cGLb6vBsIRjcKFqvxLwoxHLFagqpvfHP9vw9LC8OcHvy4jOVrW5E/rO2YMFdMl0nOm4rn+Wlp8/EqCHT+4ZethgRIjeaq6X8EoAAAAAGB42R9G4dUiTcWsYIArDIDvcx6MeoJBf5ekVSodilH+O33IyQPU2KES24oTFlgzXjeo4nTzOZSl/EKFVld4OttGYfXLNMW0wT8LUZfUrkdR0qTD1ABENAr+7j3SKHg/NT0Gx24Fcs5T5M+5kOC/VaF8LE2lNftPbXo0iLzi5lH+7cV7fBgFAAAAAAwz+8IoBFsOHHQtDQ0qHVHGMog3zOoMSXYgG0/a/dL1KspgOCTobdVoJmlR5nKVGnK3rRdVmjkWJ0sxCp2CW69RiJImHaYGwHS7kLRv2yh4txXHGE1pzrUtZ9yAqVHw5qsff1qD5w0AAAAAMCwMv1GQg5j5Imvh8nbvad7OusZC/b+N/fY+d8fkHPRBr/vW2m9cRMtH2yjUryUitSiYpUnHoBgFTYtCSP74MTUKaFEAO4YzuUAybj9nRsYo/eFsYKC9YL1KxY8nKTUqn0l8kP/FRap1u7VelWlqhG2vM7c97bPFykpKU05V2DaX+TZh5RSAPmWrQYsnfOP7OK+8ZYVPUJG9ajIRCCtmfDKNo2Nk8TIWs2hswj8JiKTXMg7AHjP0RsEexKwboCwRLQa+Qc1uK0JdtC74+/ebL7KlD3rXv2JBvq4rkM8oRB2jYJYmHRGNQtexDCGBeRSjYDpGgedXp+EXElOjgDEKYEdoLNN0MkapMyWquc2G6/bkAiO+e/bnZcqx5036QoUazqQDvGvihTTbVj8Gx6ZJyx9ZbBtN4NPTPrkBYN/zIKaTUViZpRTbN98ORgEMDMwklE9J067WnXemxaQd0wu1dgv/Oqv7maGwTnSoU/mq/GdSZE1MU+mxnJhkq0WNBzOUYeXDM/FGz2UcgL1nyI2CPTi523oJYgyCZ7Yh+VkioQ/UxQxDFqU+rXjeOLQaFSpMZKi44kSSIQG4eGgkKP+NUrVvrdsPDdUo8EDgTJRZj0zSpMPUKPDAJEbWuUp7ZqUWf9hlKM7M1k4aBWs8R6VnylWTsx6l57zv+vmsR1YyTxX1La1IU5oyyrSrxkaBgVmPwLZ5XafFJXXaXwd7cgFLuRdFy5jv+WNjd4NMf6kfXcRbv6zkLC1rpkruZZ+NJWliburKqWSLz9hmUe5OOeSZAUA/IlvKkgVa/JO3vLSeLdLiM039+JrVOSy4D7/HW1RfWqS6ZkZBUTZHWBmS9WSvZRyAfmC4jcLPZcrGDFZgFm+m1Sk6GeKz4ExJLoE1C/j8/TPeufZDA3D2IOFrCZwYo7icg1/M03+v6G1R4LDj1Bby7DhyHQXerGm8joIuTTpMjQJjvUqzH6ZpbHRMHMPePwuKfNttu0XBWUfhaIYyzjoKfDpZuZWKfx2F9LEczbA8ak9HG80ocLquoyCAUQDR8Y+DqV1mpj/k3uT3rXVZ812LlZOE3fKlG1fTyz4bSwUq8amQteXUxjlWYyv82QZA38GnH786SxX2CO88Dk1lG893X++Anso4AH3CkLcoDC+ixSNsxh8AQN8ixh4pZZevV8K7tPEgxkOTv9FkZkBZD8Shfi0lWvb4N7rAp5d9uoQZBdESmpEzicEogMHE3Cj4x6SZI1oUlHGI2yqPAOwxMAr9TKtGxWM5TxcYTvPxLGVGUjTzBA8XAAYK0RLgb+Vs0erSNKXHM5RfqNLqi1WqLhVocjxN07eDnRVoja8fMkVl+bZSH/hE3KeK1ijYXaZSl2vS4MAogMHE1Cg4wb06VXpX3DEK3gU/t1UeAdhjYBT6nNaLiujqk0ra04Dyf0V3n+cwCQAMFrKf9JESrSrd4gR8FfWlPGXGLBpLjpE1xgIKd1VzBT5ry29ilFH6NIcGPqb79KMxCs5A/orbHxtGAQwmRkbhNbu/k95y1gmxT1YehMamaPaxZu+9lkcA9hgYBQAA2AXcwcL+F4iv6zR71KLUmcX2pAXNOi3yGVWOznoGS/IZ0/gAZnVqZW3gE2GfAfxGQa774p1+GUYBDCZdjYKcQrXjjEdhbLVofa1MhYk4TS0pBX075RGAPQZGAQAA3jJiJq2RNM1o+iKvzqX0M7NtrYsFIVNz8q2mnC3N359ZF/gY71OHzyjw8RDBwApGAQwmHY2CnELVmpih2naCdzEZSnvGxG2VRwD2mP1pFFjhrD+oteczBkNIixqPq1QPfW0EwO7QuMlbEjI0q5uCkYUrnWZW8QQ1IoCX3RvCJGZWibBPHR6jYO9LeyxXvpnaAOhjQu9/pyVhR97wq0Z6m+URgD1mHxiFJq0+qNKquhbC8yKlYhZNf7PHhiWAJq1AgQX/T6rtBaw60Vym6RG8qQF7CJ+SkS9gNpqlxbAFH9k9zdd6cWYw8hJcc0FHMNDY5j41YxSCoEUBDCbawLxZpZkJi+KnInQ3YsaislDRb88nHHAN9PbLOAB7yT4wCoNUoaHy7cw25rUGYDd5vUol0YWhQJVut2tjkaZGLMpcrnpWba1ezujHNPjQBj6m+2TpLM8VvZMjwCiAIcZfXlrP+WKa9qrJ6mKlQfhip0Uq3l61A3457sA6OkOVtXV7ZWZG89mivSo7C/7dUrXNMg7AXgKj0Feg8u0MjAIYDMQCS6wsh8of2L+qUvHjSUqNyu9H+exmRaoadOkJ7bpgss8XizTJApjJm0qkAqMAhhhveanRzIgsHyFq1zcNWnyfGYP3F9srLPOZjG4XKDsxRpbcPp60F0UNzGa0jTIOwF4y1EZh/c40pQ+lKC4Kb5r9f5qWeZn3rZro/v2kRqVzk/ZvWCHOztWp5ax2zKcmHbVo7Gielv3u39mGr+ArVkSepNxcNfB2QqzGLKY65dvwlZbbD5TQtEpMVgp2HoB1sW2GMkftY02eK1HNF0V0SksoTj6t1Kl8Mcv2n6E0n7b1WJ5KT3wH4Bjliww4HtrbZthDVBt8PCmKfYyxh7olV7MuPuFfhP3eH8jIvyOs+AwAAAAAsJ/Zny0KWqPAAtCjM213z5sKYxbFxydphgWXAt7n+ELCO3vBVp2KE3HKXKu1g2z+luFLPo1hoT1zgpixJMUCWuXcWw2qfJqm3B0nyNakldH8ZpqZh2kqq/2cX1Uon7RInYJNGIURlpYLFXK78TtpGWXGwxn7YJQWDSKfWJ4cKVBFGSfQXCnRFDuuZ8yHab7Ic7ZGM8yorHY2KtoWhbDf+/NSbjeeo9IzJZ0yH9PMFAIAAAAAgDYwCu7fk75+gnZQ6mmS5/ia5RsLGbI+Wg5OeyYHKWUW5O+fzJDFflftGAhr0tqqUp4F9bMr8m8VsUJre+VIYRTUZlGXppiCLXFVBsNGadGgzSeb5u0sxZQl643zRZ7z9D2Td/rhRiH4e39eyr8fyj9Vns1SQpnKDgAAAAAAwCjo/xaE9If3GIVVKh2KUf47fZDb+i5PsUMlthVDvGG3KH5slsqPV2ld+5NgWj37COA9fmhfZUbzTi5iWjRo80nSXKZcLE2lNf5HhHzRXZ9Qwo1C8Pf+zzsdp06zifD0AgAAAADsR2AUdH8LTIyCvW97TEGYinJbxhaf/nSRiueyYkBTcNBTMK12d6IxzX7bmpbdhToZhcAAxa5p0dDJKHjSHiVfNNcnlLdlFEKuNQAAAADAPgZGQfe3wLxFIXent/NpNXj/+FigG46aVvH2nXfpkX93wrhFQUMwLRo6GQVNi4JZvnQK4P2gRQEAAAAAYLeAUdD9LTAxCnZffB6cGy/S4sMb3GvSKsYoJKjwsHsQK/ZlMkYhhI4tEhyRT+ZjFMzypVMA72cHjALGKAAAAAAAGLEvjAKfJ1kMdt1q2Yui7KBRcPr7pz71LtbC39AXJjJUXLED/BYLRjN88SV1VeEm+y07htqiEEgrQ8x6NDJFJXU6VD6D0EKWUh8vu4G9CPT5rEe/K9Oqcxh31qM8VeVMQ2Zp0SDyic96NE3ltfZvnVmP8g+U/RnmS3igr2OdyidilP6T3S7SEruIZhSs8SkqPlbyUc56NLkQ1tYCAAAAALA/2QdGgajxVY5So3GKJ1N2f/6dNAqcwHoBaZr8eCa4zsGDIuWOpWhMrAOQopRmXEAgrRLPOgriGFnKL3jXJHBaBOx1FOztwtZRMElLACef5DoKIh2R1lHQ5UsUo8DMwWNmcsZZ/oynKDPHV76MZhTcdRRk2rGOAgAAAACAnn1hFPYLXbsObRetoRoUohkSAAAAAID9DozCEAGj0AkYBQAAAACAKMAoDBEwCp2AUQAAAAAAiAKMAgAAAAAAACAAjAIAAAAAAAAgAIwCAAAAAAAAIACMAgAAAAAAACAAjAIAAAAAAAAgAIwCAAAAAAAAIMCeGYVGowFBEARBEARBUB+Ks2dGAYIgCIIgCIKg/tauG4WtrS1xwH/84x/i4BAEQRAEQRAE9Z/+67/+i/7nf/5nd4wChx/IMQsQBEEQBEEQBPWnHJPAX/ZHpSejwA/ExQ8KQRAEQRAEQVD/qheTwOnJKAAAAAAAAACGGxgFAAAAAAAAQAAYBQAAAAAAAEAAGAUAAAAAAABAABgFAAAAAAAAQAAYBQAAAAAAAEAAGAUAAAAAAABAABgFAAAAAAAAQAAYBQAAAAAAAEAAGAUAAAAAAABAABgFAAAAAAAAQAAYBQAAAAAAAEAAGAUAAAAAAABAABgFAAAAAAAAQAAYBQAAAAAAAEAAGAUAAAAAAACAD6L/D4cap3zCWtG4AAAAAElFTkSuQmCC",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "image/png": {
       "width": 600
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"img/q_learning_compare.png\",width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters and optimizations\n",
    "Learn about it on: https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
